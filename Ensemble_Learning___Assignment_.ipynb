{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "Answer: Ensemble Learning in machine learning is a technique where multiple models (often called \"weak learners\") are combined to create a stronger, more accurate predictive model. Instead of relying on a single model, ensemble methods aggregate the outputs of several models to reduce errors and improve generalization.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "‚ÄúThe wisdom of the crowd‚Äù: Just as a group of people making a decision together often performs better than an individual, combining multiple models helps balance out the weaknesses of each individual learner.\n",
        "\n",
        "Individual models may have biases or make random errors, but when combined properly, their collective prediction tends to be more robust, accurate, and stable.\n",
        "\n",
        "Types of Ensemble Methods\n",
        "\n",
        "Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Trains multiple models on different random subsets of the training data.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "Goal: Reduce variance and prevent overfitting.\n",
        "\n",
        "Boosting\n",
        "\n",
        "Models are trained sequentially, with each new model focusing on correcting the mistakes of the previous one.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "Goal: Reduce bias and improve accuracy.\n",
        "\n",
        "Stacking\n",
        "\n",
        "Combines predictions from multiple models using a meta-model (a model that learns how to best combine them).\n",
        "\n",
        "Goal: Capture complex relationships by leveraging diverse learners.\n",
        "\n",
        "**Question 2: What is the difference between Bagging and Boosting? **\n",
        "\n",
        "Answer: Bagging and Boosting are two popular ensemble learning techniques, but they differ in how models are trained and how errors are handled.\n",
        "\n",
        "üîπ Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Training approach:\n",
        "\n",
        "Multiple models are trained independently and in parallel on different random subsets of the data (sampled with replacement).\n",
        "\n",
        "Combination:\n",
        "\n",
        "Final prediction is made by majority vote (classification) or averaging (regression).\n",
        "\n",
        "Goal:\n",
        "\n",
        "Reduce variance and avoid overfitting.\n",
        "\n",
        "Example:\n",
        "\n",
        "Random Forest.\n",
        "\n",
        "üîπ Boosting\n",
        "\n",
        "Training approach:\n",
        "\n",
        "Models are trained sequentially, where each new model focuses on the errors (misclassified data points) made by the previous models.\n",
        "\n",
        "Combination:\n",
        "\n",
        "Final prediction is a weighted sum of all models.\n",
        "\n",
        "Goal:\n",
        "\n",
        "Reduce bias and improve accuracy.\n",
        "\n",
        "Example:\n",
        "\n",
        "AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
        "\n",
        "‚úÖ Key Differences Between Bagging & Boosting\n",
        "\n",
        "| Aspect            | Bagging üëú                        | Boosting üöÄ                                                 |\n",
        "| ----------------- | --------------------------------- | ----------------------------------------------------------- |\n",
        "| Training Style    | Parallel (independent models)     | Sequential (each model learns from errors of previous one)  |\n",
        "| Data Sampling     | Random subsets (with replacement) | Full dataset, but weights adjusted for misclassified points |\n",
        "| Error Handling    | Reduces **variance**              | Reduces **bias**                                            |\n",
        "| Model Combination | Majority vote / averaging         | Weighted sum                                                |\n",
        "| Tendency          | Prevents overfitting              | Can overfit if not tuned properly                           |\n",
        "| Example Algorithm | Random Forest                     | AdaBoost, XGBoost                                           |\n",
        "\n",
        "\n",
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        " Answer:üîπ Bootstrap Sampling\n",
        "\n",
        "Bootstrap sampling is a statistical technique where we create new datasets (called bootstrap samples) by randomly selecting data points from the original dataset with replacement.\n",
        "\n",
        "Each bootstrap sample is the same size as the original dataset, but because of replacement, some records may appear multiple times while others may not appear at all.\n",
        "\n",
        "On average, about 63% of the original data points appear in each bootstrap sample, and the remaining are called out-of-bag (OOB) samples.\n",
        "\n",
        "üîπ Role in Bagging (e.g., Random Forest)\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling plays a key role:\n",
        "\n",
        "Diversity of Models\n",
        "\n",
        "Each decision tree is trained on a different bootstrap sample of the data.\n",
        "\n",
        "This introduces variation, so trees don‚Äôt all make the same mistakes.\n",
        "\n",
        "Reduced Overfitting\n",
        "\n",
        "Individual decision trees can overfit, but averaging predictions across many diverse trees reduces variance and improves generalization.\n",
        "\n",
        "Out-of-Bag (OOB) Error Estimation\n",
        "\n",
        "Since ~37% of the data is left out of each bootstrap sample, these OOB samples can be used as a validation set to estimate model performance without needing a separate test set.\n",
        "\n",
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        " Answer: üîπ Out-of-Bag (OOB) Samples\n",
        "\n",
        "In bootstrap sampling, each new dataset is created by sampling with replacement from the original dataset.\n",
        "\n",
        "On average, about 63% of the data points are included in each bootstrap sample, while the remaining ~37% are left out.\n",
        "\n",
        "These left-out data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "OOB samples are different for each tree in an ensemble like a Random Forest.\n",
        "\n",
        "üîπ OOB Score\n",
        "\n",
        "The OOB score is a performance metric that uses OOB samples to evaluate the model:\n",
        "\n",
        "For each data point, check which trees did not use it in their bootstrap sample.\n",
        "\n",
        "Use only those trees to predict the label for that data point.\n",
        "\n",
        "Compare the predicted label with the true label.\n",
        "\n",
        "Aggregate results across all data points to compute the OOB error (1 ‚Äì OOB score).\n",
        "\n",
        "üîπ Why OOB Score is Useful?\n",
        "\n",
        "Acts like a built-in cross-validation technique.\n",
        "\n",
        "Provides an unbiased estimate of model performance without needing a separate validation set.\n",
        "\n",
        "Saves computational cost since evaluation happens during training.\n",
        "\n",
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        " Answer: Feature importance tells us how much each feature contributes to a model‚Äôs predictions. The way it is calculated differs between a single Decision Tree and an ensemble like Random Forest.\n",
        "\n",
        "üîπ Feature Importance in a Single Decision Tree\n",
        "\n",
        "A Decision Tree splits nodes based on the feature that maximizes some impurity reduction (e.g., Gini Index or Entropy for classification, MSE reduction for regression).\n",
        "\n",
        "The importance of a feature is calculated as:\n",
        "\n",
        "Importance(feature)\n",
        "\n",
        "= Sum¬†of¬†impurity¬†reduction¬†over¬†all¬†features/\n",
        "Total¬†reduction¬†in¬†impurity¬†from¬†splits¬†using¬†that¬†feature\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Features used near the root of the tree tend to get higher importance because they affect more samples.\n",
        "\n",
        "The result may be biased if the tree is small or if some features dominate early splits.\n",
        "\n",
        "üîπ Feature Importance in a Random Forest\n",
        "\n",
        "A Random Forest is an ensemble of many Decision Trees trained on bootstrap samples with feature randomness (each split considers only a random subset of features).\n",
        "\n",
        "Feature importance is computed as the average impurity reduction contributed by each feature across all trees in the forest.\n",
        "\n",
        "This makes the importance measure:\n",
        "\n",
        "More stable and reliable than a single tree.\n",
        "\n",
        "Less biased towards features that happen to dominate in one tree.\n",
        "\n",
        "Reflective of a feature‚Äôs usefulness on average.\n",
        "\n",
        "‚úÖ Key Comparison\n",
        "\n",
        "| Aspect      | Single Decision Tree üå≥                 | Random Forest üå≤üå≤üå≤                                |\n",
        "| ----------- | --------------------------------------- | --------------------------------------------------- |\n",
        "| Calculation | Based on impurity reduction in one tree | Averaged impurity reduction across many trees       |\n",
        "| Stability   | Can vary a lot if the tree changes      | More stable due to averaging                        |\n",
        "| Bias        | May favor features used near the root   | Less biased since many trees contribute             |\n",
        "| Reliability | Less reliable (high variance)           | More reliable (low variance, better generalization) |\n"
      ],
      "metadata": {
        "id": "kCjHmYcZjx9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to: ‚óè Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ‚óè Train a Random Forest Classifier ‚óè Print the top 5 most important features based on feature importance scores. (Include your Python code and output in the code box below.) **\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "xIKtyxiWmf4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx0wHqPSjR3u",
        "outputId": "367704be-dadc-4f28-8f0b-ad4519c0a08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "1. worst area: 0.1394\n",
            "2. worst concave points: 0.1322\n",
            "3. mean concave points: 0.1070\n",
            "4. worst radius: 0.0828\n",
            "5. worst perimeter: 0.0808\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Sort features by importance\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to: ‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset ‚óè Evaluate its accuracy and compare with a single Decision Tree (Include your Python code and output in the code box below.)**\n",
        "\n",
        " Answer:  "
      ],
      "metadata": {
        "id": "xSa59yVXm9EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1) Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# 2) Train a Bagging Classifier (uses many Decision Trees)\n",
        "bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(\"Single Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy   :\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yo4IpC6mqlu",
        "outputId": "8d1cd474-02fc-4a7f-fc35-edf28f38edc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy   : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to: ‚óè Train a Random Forest Classifier ‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV ‚óè Print the best parameters and final accuracy (Include your Python code and output in the code box below.)**\n",
        "\n"
      ],
      "metadata": {
        "id": "BCU6_ujgn-AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train & test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],   # number of trees\n",
        "    'max_depth': [None, 3, 5, 7]      # depth of trees\n",
        "}\n",
        "\n",
        "# Use GridSearchCV\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and best model\n",
        "best_params = grid.best_params_\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy :\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY_fYsGbnGB_",
        "outputId": "5a941c51-48e8-4dff-9289-afc5dd6186e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Accuracy : 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to: ‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ‚óè Compare their Mean Squared Errors (MSE) (Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "45cgDcOEoYpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor (uses many Decision Trees)\n",
        "bagging = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "mse_bag = mean_squared_error(y_test, bagging.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "mse_rf = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE :\", round(mse_bag, 4))\n",
        "print(\"Random Forest MSE     :\", round(mse_rf, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn6Z8DoIoLoY",
        "outputId": "1ed4fc8c-a253-4941-c7ea-428c24876b57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE : 0.2579\n",
            "Random Forest MSE     : 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ‚óè Choose between Bagging or Boosting ‚óè Handle overfitting ‚óè Select base models ‚óè Evaluate performance using cross-validation ‚óè Justify how ensemble learning improves decision-making in this real-world context. **\n",
        "\n",
        "\n",
        "Answer:1. Choose between Bagging or Boosting\n",
        "\n",
        "Bagging (e.g., Random Forest): Good when the main issue is overfitting and high variance (e.g., decision trees are too unstable).\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM): Better when the dataset is complex and we need to reduce bias (capture difficult patterns).\n",
        "üëâ For financial loan default prediction (imbalanced + complex patterns), I‚Äôd start with Boosting because it usually gives higher accuracy.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "Use cross-validation to monitor performance on unseen folds.\n",
        "\n",
        "Apply regularization in Boosting (e.g., learning rate, max_depth).\n",
        "\n",
        "Use early stopping to avoid too many iterations.\n",
        "\n",
        "For Bagging/Random Forest ‚Üí limit tree depth, use fewer features per split.\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "For Bagging: Base model = Decision Tree.\n",
        "\n",
        "For Boosting: Base model = Shallow Decision Tree (weak learner).\n",
        "üëâ Shallow trees (depth 3‚Äì5) are common because they prevent overfitting and work well in ensembles.\n",
        "\n",
        "4. Evaluate Performance using Cross-Validation\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation since data is imbalanced (default vs non-default).\n",
        "\n",
        "Metrics: AUC-ROC, Precision-Recall, F1-score (not just accuracy, since defaults are rare).\n",
        "\n",
        "Compare Bagging vs Boosting performance and select the better one.\n",
        "\n",
        "5. Justify Ensemble Learning in Real-World Context\n",
        "\n",
        "Single models may miss patterns (e.g., income vs spending behavior vs late payments).\n",
        "\n",
        "Ensembles combine many weak learners ‚Üí stronger, more stable predictions.\n",
        "\n",
        "Boosting focuses on the hard-to-predict customers (those likely to default but look normal).\n",
        "\n",
        "More accurate predictions ‚Üí better risk assessment ‚Üí fewer bad loans ‚Üí increased profit + reduced risk for the bank.\n",
        "\n"
      ],
      "metadata": {
        "id": "R9VnPZv0o_Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUSPmnjPoloG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}